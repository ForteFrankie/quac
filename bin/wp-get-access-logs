#!/usr/bin/env python
# -*- coding: utf-8 -*-

'Update local copy of Wikimedia access logs.'

# Copyright (c) 2012-2013 Los Alamos National Security, LLC, and others.

help_epilogue = '''
Logs only go to stdout, so as to capture the rsync chatter as well.

Note that we use rsync to download Wikipedia stuff, which isn't supported by
the Wikimedia site, so you have to use a mirror. See the list of mirrors:
<http://meta.wikimedia.org/wiki/Mirroring_Wikimedia_project_XML_dumps>

Notes:

* If a log file is changed, your local copy will be updated to match. This
  probably shouldn't happen, so investigate the situation if it does.
  (However, deleted log files will not also be deleted locally; e.g., you are
  OK if the mirror you are using goes into a zombie state.)

* This script will download several terabytes of data. Be patient and be
  courteous.

* --verify also takes a long time and gives no output if all's well. If you're
    worried about progress, you can use lsof to see what file is currently
    being checked.
'''

from datetime import datetime
import glob
import io
import os.path
import subprocess

import pickle_glue
import quacpath
import rsync
import testable
import time_
import u
import wikimedia

l = u.l
c = u.c


### Setup ###

ap = u.ArgumentParser(description=__doc__, epilog=help_epilogue)
gr = ap.add_argument_group('arguments')
gr.add_argument('--config',
                help='location of config file',
                default=u.CONFIG_DEFAULT,
                metavar='FILE')
gr.add_argument('--verify',
                action='store_true',
                help='verify MD5sums of existing files instead of downloading')
gr.add_argument('--metadata',
                action='store_true',
                help='skip download; just rebuild metadata')


### Main ###

def main():
   l.info('Wikimedia access logs in %s' % (log_dir))
   if (args.verify):
      l.info('mode: verify')
      subprocess.call('find %s -name md5sums.txt -exec sh -c "cd \$(dirname {}) && md5sum --check --quiet md5sums.txt || echo MD5 error in {}" \;' % (log_dir), shell=True)
   else:
      l.info('mode: update')
      if (not args.metadata):
         l.info('bandwidth limit is %d KB/s' % (bwlimit))
         # FIXME: awkward to specify --include * simply to override --exclude *.
         rsync.all(mirror_url, log_dir, bwlimit, args.verbose)
      l.info('rebuilding metadata')
      metadata_build()
   l.info('done')


### Other functions ###

def metadata_build():
   mdfile = pickle_glue.File('%s/metadata' % (log_dir),
                             writable=True, default=dict())
   files = glob.glob('%s/20*/*/projectcounts-*' % (log_dir))
   counts = u.defaultdict_recursive() # triples: project, timestamp, view count
   for file_ in files:
      read_file(file_, counts)
   md = dict()
   for (project, counts) in counts.iteritems():
      date_min = min(ts.date() for ts in counts.iterkeys())
      date_max = max(ts.date() for ts in counts.iterkeys())
      md[project] = dict()
      for dt in time_.dateseq(date_min, date_max):
         md[project][dt.date()] = 0
      for (ts, ct) in counts.iteritems():
         md[project][ts.date()] += ct
   mdfile.data['projects'] = md
   mdfile.commit()

def read_file(file_, counts):
   # Use replacement for errors because some of the files have UTF-8
   # encoding problems.
   fp = io.open(file_, 'r', encoding='utf8', errors='replace')
   ts = wikimedia.timestamp_parse(file_)
   for (i, line) in enumerate(fp):
      if (line.find(u'ï¿½') >= 0):
         l.debug('invalid UTF-8 in %s, ignoring line %d' % (file_, i+1))
         continue
      row = line.split()
      try:
         counts[row[0]][ts] = int(row[2])
      except IndexError:
         l.warning('bad input line in %s, aborting file read' % (file_))
         return


### Bootstrap ###

try:

   args = u.parse_args(ap)
   u.configure(args.config)
   u.logging_init('wpacc')

   bwlimit = c.getint('wkpd', 'bandwidth_limit')
   mirror_url = c.get('wkpd', 'access_log_url')

   log_dir = c.getpath('wkpd', 'access_log_dir')
   if (not os.path.isdir(log_dir)):
      u.abort('%s is not a directory or does not exist' % (log_dir))

   if (__name__ == '__main__'):
      main()

except testable.Unittests_Only_Exception:
   testable.register('')


#  LocalWords:  projectcounts
