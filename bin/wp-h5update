#!/usr/bin/env python3

'Update an HDF5 time series file with content from Wikipedia pageview files.'

# Copyright Â© Los Alamos National Security, LLC, and others.

import quacpath
import u
import testable
import wikimedia

l = u.l


### Setup ###

ap = u.ArgumentParser(description=__doc__)
gr = ap.default_group
gr.add_argument('--compress',
                action='store_true',
                help='Rewrite and compress file after updating')
gr.add_argument('outfile',
                metavar='OUTFILE',
                help='HDF5 file cluster to create or update')
gr.add_argument('pv_files',
                metavar='PAGECOUNT',
                nargs='+',
                help='pagecount files to add to metadata')


### Script ###

def main():
   h5c = h5.Sharded(args.outfile, shard_ct=c['wkpd']['hashmod'])
   h5c.close(args.compress)


### Bootstrap ###

try:
   args = u.parse_args(ap)
   u.configure(None)
   if (__name__ == '__main__'):
      main()
except testable.Unittests_Only_Exception:
   testable.register('')
