#!/usr/bin/env python3

'Update an HDF5 time series shard cluster with Wikipedia pageview files.'

# Copyright Â© Los Alamos National Security, LLC, and others.

# Data facts related to performance:
#
#   * The Wikipedia datafiles contain about 8M lines each.
#   * There are 720 data files in a 30-day month.
#   * About 25% of the lines in each data file are removed by egrep before
#     they get to the Python code.
#   * Therefore, we need to process about 4.3G lines per month of data.
#   * To do so in 24 hours, we need to move 50,000 lines per second.
#
# It turns out that HDF5 via h5py has at least two significant bottlenecks.
#
#   1. Setting a single element in a dataset (e.g., dataset[i] = x) is,
#      despite assertions to the contrary in the documentation, rather slow. I
#      believe this is because h5py reads the entire dataset into a NumPy
#      array (of about 4KB in our case), sets the data point, and writes the
#      dataset back.
#
#      We largely mitigate this problem by reading out an article's complete
#      vector, updating all new data points, and writing it back as a unit.
#
#   2. Retrieving and creating datasets is relatively slow. Some very
#      preliminary testing yields on the order of 10K such operations per
#      second.
#
# To make the latter problem tractable, we leverage the fact that article hits
# follow a long-tail distribution. Most articles have only a handful of hits
# in a month (perhaps because they are not real articles) and can therefore be
# safely discarded; even if such articles carried a useful signal, relying on
# them would be risky due to noise and the opportunity for manipulation. (The
# threshold is configured at wkpd.keep_threshold.) In the bulk-loading
# situation, which is of the most concern for performance, these monthly hit
# counts are known without needing to reference any data from the HDF5 file,
# and article vectors which do not pass the threshold can be discarded without
# writing them at all.
#
# Thus, we use the following HDF5 write strategies. Recall that the --compress
# switch indicates an HDF5 data file is complete (i.e., the month is closed).
#
#   Is --compress specified?
#   |   Was the data file empty when opened?
#   |   |
#   |   |    Strategy
#   |   |    |  Write cycle
#   |   |    |  |                  Pruning
#   --- ---  -  -----------------  -----------------------------------------
#
#   no  no   0  read-modify-write  none
#   no  yes  1  write-only         none
#   yes no   2  read-modify-write  visit whole data file, delete pruned rows
#   yes yes  3  write-only         discard pruned rows before writing
#
# When is each strategy used? (Note that ordering differs.)
#
#   3. This is the fastest data path, used for bulk loading of full months. It
#      avoids both HDF5 lookups/reads and writing most article vectors (which,
#      due to the long-tail distribution of traffic, do not pass the
#      threshold).
#
#   0. This is the standard daily incremental update strategy. This operates
#      at around 10K articles per second, enough to process one day's worth of
#      data in a few hours.
#
#   1. This happens for the daily update the first time it is run each month.
#      Again, performance is acceptable due to the small number of input data.
#
#      This strategy is also important for bulk loading, in that it is used
#      for the last, incomplete month of the bulk load. This month should not
#      be included in a synchronized parallel bulk loading job, because it
#      will be much slower than the other months that can use strategy 3.
#
#   2. This happens for the daily update the last time it is run each month.
#      Because it must process the entire file, it takes significantly longer
#      than most daily updates. Care must be taken that subsequent day's
#      updates do not overlap, causing corruption.


import gzip
import itertools
import heapq
import os
import sys
import time

import numpy as np

import quacpath
import h5
import u
import time_
import testable
import wikimedia

c = u.c
l = u.l


### Setup ###

ap = u.ArgumentParser(description=__doc__)
gr = ap.default_group
gr.add_argument('--compress',
                action='store_true',
                help='Rewrite and compress shards after updating')
gr.add_argument('--config',
                metavar='FILE',
                help='Configuration file')
gr.add_argument('--limit',
                metavar='N',
                type=int,
                default=sys.maxsize,
                help='Stop processing after saving this many time series')
gr.add_argument('--nomonthcheck',
                action='store_true',
                help="Don't verify that all files are from the same month")
gr.add_argument('outfile',
                metavar='OUTFILE',
                help='HDF5 sharded file cluster to create or update')
gr.add_argument('pv_files',
                metavar='PAGECOUNT',
                nargs='+',
                help='pagecount files to add')


### Script ###

def main():
   l.info('starting')
   args.pv_files.sort()
   args.keep_threshold = int(c['wkpd']['keep_threshold'])
   if (len(args.pv_files) > 0):
      ts = wikimedia.timestamp_parse(args.pv_files[0])
      args.vector_len = time_.hours_in_month(ts)
   month_last = None
   for file_ in args.pv_files:
      if (not os.path.isfile(file_)):
         u.abort('%s is not a file' % file_)
      try:
         ts = wikimedia.timestamp_parse(file_)
         month = ts.strftime('%Y%m')
      except ValueError:
         u.abort('%s is not named like a Wikimedia pagecount file' % file_)
      if (not args.nomonthcheck
          and month_last is not None
          and month != month_last):
         u.abort('not from month %s: %s' % (month, file_))
      month_last = month
   h5s = h5.Sharded(args.outfile, shard_ct=int(c['wkpd']['hashmod']))
   l.info('opened %s with %d shards; %d hours in %s'
          % (h5s.filename, h5s.shard_ct, time_.hours_in_month(ts), month))
   process_files(h5s, args.pv_files)
   if (not args.compress):
      h5s.close()
   else:
      prune(h5s)
      l.info('compressing')
      h5s.close_and_compress()
   l.info('done')

def process_files(h5s, files):
   start = time.time()
   line_ct = 0
   valid_hours = np.empty(args.vector_len, dtype=np.float32)
   valid_hours_ds = h5s.shards[0].require_dataset(b'/metadata/valid',
                                                  (args.vector_len,), np.float32,
                                                  fillvalue=np.nan, exact=True)
   valid_hours_ds.read_direct(valid_hours)
   args.file_empty_p = (np.nansum(valid_hours) == 0)
   args.eager_prune_p = args.compress
   l.info('write strategy %d (eager prune=%d, empty=%d), keep threshold %d'
          % (args.eager_prune_p * 2 + args.file_empty_p,
             args.eager_prune_p, args.file_empty_p, args.keep_threshold))
   write_ct = 0
   total_ct = 0
   for ((proj, url), gr) in itertools.groupby(heapq.merge(*(read_file(f)
                                                            for f in files)),
                                              key=lambda i: i[:2]):
      total_ct += 1
      (_, sh) = h5s.shard_get(url)
      av = np.zeros(args.vector_len, dtype=np.float32)
      av_ds = None
      av_ds_path = b'/weirdal/' + proj + b'/' + url
      if (not args.file_empty_p):
         av_ds = sh.require_dataset(av_ds_path, (args.vector_len,), np.float32,
                                    fillvalue=0, exact=True)
         av_ds.read_direct(av)
      for (_, _, hour_offset, count) in gr:
         valid_hours[hour_offset] = 1
         av[hour_offset] = count
         line_ct += 1
      if (args.eager_prune_p and av.sum() < args.keep_threshold):
         if (av_ds is not None):
            del av_ds.parent[url]
      else:
         write_ct += 1
         if (av_ds is None):
            try:
               av_ds = sh.create_dataset(av_ds_path, data=av)
            except RuntimeError as x:
               u.abort("Can't create %s: %s" % (av_ds_path, x))
         else:
            av_ds[:] = av
      if (write_ct >= args.limit):
         break
   valid_hours_ds[:] = valid_hours
   time_used = time.time() - start
   l.info('read %s lines in %s (%d lines/s)'
          % (line_ct + 1, u.fmt_seconds(time_used), (line_ct + 1) / time_used))
   l.info('%d of %d URLs saved (%.1f%%, %d total/s)'
          % (write_ct, total_ct, 100 * write_ct / total_ct,
             total_ct / time_used))

def prune(h5s):
   if (args.file_empty_p and args.eager_prune_p):
      l.info('no pruning pass needed for write strategy 3')
      return
   l.info('pruning, keep threshold %d' % args.keep_threshold)
   keep_ct = 0
   total_ct = 0
   start = time.time()
   # If we keep a buffer instead of allocating a new array each time we
   # compute a sum, that's about 60% faster than the simpler approach. Keeping
   # the sum in an attribute would save up to 40% on top of that, at the
   # expense of keeping it up to date. That's with the attribute already in
   # memory; I didn't test fetching it from the file. My guess is that the
   # actual deletion will dominate, so I didn't pursue this more deeply,
   # though I didn't profile that.
   buf = np.empty(args.vector_len, dtype=np.float32)
   for (i, shard) in h5s.shards_valid:
      if (b'weirdal' not in shard):
         l.warning("shard %d is empty; can't prune it" % i)
      else:
         for (proj, gr) in shard[b'weirdal'].items():
            for (url, ds) in list(gr.items()):
               total_ct += 1
               ds.read_direct(buf)
               if (buf.sum() >= args.keep_threshold):
                  keep_ct += 1
               else:
                  del gr[url]
   time_used = time.time() - start
   l.info('%d of %d URLs kept in %s (%.1f%%, %d analyzed/s)'
          % (keep_ct, total_ct, u.fmt_seconds(time_used),
             100 * keep_ct / total_ct, total_ct / time_used))

def read_file(file_):
   try:
      byte_ct = os.stat(file_).st_size
      ts = wikimedia.timestamp_parse(file_)
      hour_offset = time_.hour_offset(ts)
      # Note that we filter out URLs:
      #
      # 1. In non-Wikipedia projects (Wiktionary, Wikibooks, etc.). Such
      #    projects have a dot in their project code.
      #
      # 2. With "funny" characters. This is everything except:
      #
      #      - ASCII alphanumeric
      #      - the rest of the unreserved set, except for dot: -_~
      #      - some of the reserved set: !*();@,
      #      - percent (to allow encoded URLs through)
      #
      #    Notably, this excludes articles:
      #
      #      - in non-main namespaces (title contains a colon)
      #      - with a slash in the title, such as "Input/output"
      #      - accessed with non-percent-encoded high Unicode characters
      #
      #    See:
      #
      #      http://en.wikipedia.org/wiki/Percent-encoding
      #      http://en.wikipedia.org/wiki/Wikipedia:Naming_conventions_%28technical_restrictions%29
      fp = u.zcat(file_,
                  r"zcat '%s' | egrep '^[a-z]+ [-A-Za-z0-9_~!*();@,%%]+ '")
      #l.info('opened %s' % file_)
      badline_ct = 0
      badline_last = None
      start = time.time()
      for (i, line) in enumerate(fp):
         try:
            (proj, url, count, _) = line.split(b' ')
            yield (proj, url, hour_offset, int(count))
         except ValueError as x:
            # Ignore lines that don't parse. Some files have thousands of
            # these (pagecounts-20130201-010000.gz), and many files have at
            # least one (all of February 2013).
            badline_ct += 1
            badline_last = i + 1
      if (badline_ct > 0):
         l.warning('%s: %d lines with parse errors skipped (last was %d)'
                   % (file_, badline_ct, badline_last))
      time_used = time.time() - start
      #l.debug('read %d lines and %s in %s (%d lines/s, %s/s)'
      #        % (i + 1, u.fmt_seconds(time_used), u.fmt_bytes(byte_ct),
      #           (i + 1) / time_used, u.fmt_bytes(byte_ct / time_used)))
   except (EOFError, IOError) as x:
      l.warning('%s: error reading file, skipped: %s' % (file_, str(x)))

def update_metadata(h5c):
   pass


### Bootstrap ###

try:
   args = u.parse_args(ap)
   u.configure(args.config)
   u.logging_init('h5upd')
   if (__name__ == '__main__'):
      main()
except testable.Unittests_Only_Exception:
   testable.register('')
