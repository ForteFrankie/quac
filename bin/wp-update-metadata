#!/usr/bin/env python3

'Given a set of pageview files, update a metadata file.'

# Copyright (c) 2012-2013 Los Alamos National Security, LLC, and others.

import collections
import datetime
import gzip
import os
from pprint import pprint

import numpy as np

import quacpath
import math_
import pickle_glue
import testable
import time_
import u
import wikimedia
l = u.l


### Setup ###

ap = u.ArgumentParser(description=__doc__)
gr = ap.default_group
gr.add_argument('outfile',
                metavar='OUTFILE',
                help='metadata file to create or update')
gr.add_argument('pv_files',
                metavar='PAGEVIEWS',
                nargs='+',
                help='pageview files to add to metadata')


### Globals ###

# minimum and maximum timestamps seen so far
date_min = time_.date_max
date_max = time_.date_min

# latest mtime seen so far
mtime_max = 0


### Main ###

def main():
   u.logging_init('wpmtd')
   l.debug('starting')
   mdp = metadata_pkl_open(args.outfile)
   md = mdp.data
   for filename in args.pv_files:
      pv_process(filename, md)
   l.debug('writing %s' % (args.outfile))
   mdp.commit()
   metadata_vector_dump(md)
   l.debug('setting mtime')
   # Set metadata's mtime to two microseconds beyond mtime_max. This is
   # because some Linux filesystems (e.g., ext4) store timestamps to
   # nanosecond resolution, but Python can only set to microsecond resolution.
   # For example, copying the mtime 2013-11-12 14:46:35.567961144 will result
   # in 2013-11-12 14:46:35.567961000, which is earlier; therefore,
   # unnecessary files may be processed. Adding a small interval guarantees
   # that the metadata file is the same or newer. We use two microseconds
   # rather than one because I saw (but couldn't reproduce) an example of make
   # not believing the metadata file to be newer, which I think is
   # attributable to a rounding error of some kind. (This assumes that the
   # earliest file processed in the *next* run is at least two microseconds
   # newer than the latest file in this run; to violate this would require
   # processing around a half million files per second, which is pretty fast
   # even if you are just copying or moving the logfiles.)
   os.utime(args.outfile, (mtime_max + 2e-6, mtime_max + 2e-6))
   l.debug('done')


### Functions and classes ###

def metadata_vector_dump(md):
   '''Make the Date_Vector extract of the metadata and dump it to a pickle
      file. The main hourly file is canonical, so we don't lock the daily one
      or try to reload it.'''
   totals = dict()
   # Compute mask. If a day has more than 12 hours of data in English
   # Wikipedia, we call it a valid day's data.
   en = md['projects']['en']
   date_min = min(en.keys())
   date_max = max(en.keys())
   mask = math_.Date_Vector.zeros(date_min, date_max, dtype=np.bool)
   for (i, d) in enumerate(time_.dateseq(date_min, date_max)):
      if (d in en and len(en[d]['hours']) >= 12):
         mask[i] = True
   totals['mask'] = mask
   # Compute totals for each project
   totals['projects'] = dict()
   for (project, dates) in md['projects'].items():
      series = math_.Date_Vector.zeros(date_min, date_max, dtype=np.float32)
      for (i, d, _) in series.enumerated:
         series[i] = dates.get(d, {'total': 0})['total']
      totals['projects'][project] = { 'total': sum(series),
                                      'series': series }
   u.pickle_dump(args.outfile + '.total', totals)

def metadata_pkl_open(filename):
   # Include the metadata file in the mtime_max computation. If we are
   # re-processing some old data, we don't want to re-do everything newer than
   # that in the next make run.
   global mtime_max
   mtime_max = max(mtime_max, os.stat(filename).st_mtime)
   return pickle_glue.File(filename, writable=True,
                           default={ 'badfiles': set(), 'projects': dict() })

def pv_process(filename, md):
   'Compute and save metadata for the given pageview file.'
   l.debug('reading %s' % (filename))
   md['badfiles'].discard(filename)  # maybe not bad this time?
   ts = wikimedia.timestamp_parse(filename)
   fp = gzip.open(filename, 'rb')
   counts = collections.Counter()
   try:
      # Include mtime even for skipped files in the mtime_max computation,
      # because even if they're bogus input files, they're still input files.
      # (For example, if a bogus file is the latest modified input file, we
      # don't want to process it again on the next run.)
      global mtime_max
      mtime_max = max(mtime_max, os.stat(filename).st_mtime)
      badline_ct = 0
      badline_last = None
      for (i, line) in enumerate(fp):
         try:
            (proj, _, count, _) = line.split(' ')
            counts[proj] += int(count)
         except ValueError as x:
            # Ignore lines that don't parse. Don't log because some files have
            # thousands of these (e.g., pagecounts-20130201-010000.gz). We do
            # warn with a total later if it's greater than 0.
            #
            # We used to give up on a file after a single parse error, but
            # this turned out to be too aggressive -- for example, we'd lose
            # all of February and half of March 2013.
            badline_ct += 1
            badline_last = i + 1
   except (EOFError, IOError) as x:
      l.warning('%s: error reading file, skipped: %s' % (filename, str(x)))
      md['badfiles'].add(filename)
      return

   if (badline_ct > 0):
      l.warning('%s: %d lines with parse errors skipped (last was %d)'
                % (filename, badline_ct, badline_last))

   for (proj, count) in counts.items():
      update(md['projects'], ts, proj, count)

def update(md, timestamp, proj, count):
   '''e.g.:

      >>> d = dict()
      >>> update(d, datetime.datetime(2013, 10, 31, 12, 1, 1), 'en', 100)
      >>> pprint(d)
      {'en': {datetime.date(2013, 10, 31): {'hours': {12: 100}, 'total': 100}}}
      >>> update(d, datetime.datetime(2013, 10, 31, 13), 'en', 10)
      >>> pprint(d)
      {'en': {datetime.date(2013, 10, 31): {'hours': {12: 100, 13: 10},
                                            'total': 110}}}
      >>> update(d, datetime.datetime(2013, 10, 30, 12), 'en', 200)
      >>> pprint(d)
      {'en': {datetime.date(2013, 10, 30): {'hours': {12: 200}, 'total': 200},
              datetime.date(2013, 10, 31): {'hours': {12: 100, 13: 10},
                                            'total': 110}}}

      Setting a new value for an hour already in the data silently replaces
      the old value:

      >>> update(d, datetime.datetime(2013, 10, 31, 12, 2, 2), 'en', 300)
      >>> pprint(d)
      {'en': {datetime.date(2013, 10, 30): {'hours': {12: 200}, 'total': 200},
              datetime.date(2013, 10, 31): {'hours': {12: 300, 13: 10},
                                            'total': 310}}}'''
   date = timestamp.date()
   hour = timestamp.hour
   md.setdefault(proj, dict())
   md[proj].setdefault(date, { 'total': None, 'hours': dict() })
   md[proj][date]['hours'][hour] = count
   md[proj][date]['total'] = sum(md[proj][date]['hours'].values())

### Bootstrap ###

try:
   args = u.parse_args(ap)
   if (__name__ == '__main__'):
      main()
except testable.Unittests_Only_Exception:
   testable.register('')
