#!/usr/bin/env python

'Given a set of pageview files, update a metadata file.'

# Copyright (c) 2012-2013 Los Alamos National Security, LLC, and others.

import collections
import datetime
import gzip
import os
from pprint import pprint

import quacpath
import pickle_glue
import testable
import time_
import u
import wikimedia
l = u.l


### Setup ###

ap = u.ArgumentParser(description=__doc__)
gr = ap.default_group
gr.add_argument('outfile',
                metavar='OUTFILE',
                help='metadata file to create or update')
gr.add_argument('pv_files',
                metavar='PAGEVIEWS',
                nargs='+',
                help='pageview files to add to metadata')


### Globals ###

# minimum and maximum timestamps seen so far
date_min = time_.date_max
date_max = time_.date_min

# latest mtime seen so far
mtime_max = 0


### Main ###

def main():
   u.logging_init('wpmtd')
   l.debug('starting')
   mdp = metadata_pkl_open(args.outfile)
   md = mdp.data
   for filename in args.pv_files:
      pv_process(filename, md)
   l.debug('writing %s' % (args.outfile))
   mdp.commit()
   metadata_daily_dump(md)
   l.debug('setting mtime')
   # Set metadata's mtime to two microseconds beyond mtime_max. This is
   # because some Linux filesystems (e.g., ext4) store timestamps to
   # nanosecond resolution, but Python can only set to microsecond resolution.
   # For example, copying the mtime 2013-11-12 14:46:35.567961144 will result
   # in 2013-11-12 14:46:35.567961000, which is earlier; therefore,
   # unnecessary files may be processed. Adding a small interval guarantees
   # that the metadata file is the same or newer. We use two microseconds
   # rather than one because I saw (but couldn't reproduce) an example of make
   # not believing the metadata file to be newer, which I think is
   # attributable to a rounding error of some kind. (This assumes that the
   # earliest file processed in the *next* run is at least two microseconds
   # newer than the latest file in this run; to violate this would require
   # processing around a half million files per second, which is pretty fast
   # even if you are just copying or moving the logfiles.)
   os.utime(args.outfile, (mtime_max + 2e-6, mtime_max + 2e-6))
   l.debug('done')


### Functions and classes ###

def metadata_daily_dump(md):
   '''Make the daily extract of the metadata and dump it to a pickle file. The
      main hourly file is canonical, so we don't lock the daily one or try to
      reload it.'''
   md_daily = dict()
   md_daily['badfiles'] = md['badfiles']
   md_daily['projects'] = dict()
   for (proj, pdata) in md['projects'].iteritems():
      md_daily['projects'][proj] = dict()
      pdata_d = md_daily['projects'][proj]
      for (date, ddata) in pdata.iteritems():
         pdata_d[date] = dict()
         ddata_d = pdata_d[date]
         ddata_d['total'] = ddata['total']
         ddata_d['hours_len'] = len(ddata['hours'])
   u.pickle_dump(args.outfile + '.daily', md_daily)

def metadata_pkl_open(filename):
   return pickle_glue.File(filename, writable=True,
                           default={ 'badfiles': set(), 'projects': dict() })

def pv_process(filename, md):
   'Compute and save metadata for the given pageview file.'
   l.debug('reading %s' % (filename))
   ts = wikimedia.timestamp_parse(filename)
   fp = gzip.open(filename, 'rb')
   counts = collections.Counter()
   try:
      # Include mtime even for skipped files in the mtime_max computation,
      # because even if they're bogus input files, they're still input files.
      # (For example, if a bogus file is the latest modified input file, we
      # don't want to process it again on the next run.)
      global mtime_max
      mtime_max = max(mtime_max, os.stat(filename).st_mtime)
      for (i, line) in enumerate(fp):
         try:
            (proj, _, count, _) = line.split(' ')
            counts[proj] += int(count)
         except ValueError as x:
            # We give up on a file after a single parse error. Indeed, this is
            # aggressive. The right thing to do would be one of (a) lose the
            # parseability guarantee for files that pass through metadata, or
            # (b) patch together a repaired version. I'm currently too lazy
            # for that.
            x.args = ('bad line %d: %s' % (i+1, x.args[0]),)
            raise x
   except (EOFError, IOError, ValueError) as x:
      l.warning('%s: skipping file: %s' % (filename, str(x)))
      md['badfiles'].add(filename)
      return

   for (proj, count) in counts.iteritems():
      update(md['projects'], ts, proj, count)

def update(md, timestamp, proj, count):
   '''e.g.:

      >>> d = dict()
      >>> update(d, datetime.datetime(2013, 10, 31, 12, 1, 1), 'en', 100)
      >>> pprint(d)
      {'en': {datetime.date(2013, 10, 31): {'hours': {12: 100}, 'total': 100}}}
      >>> update(d, datetime.datetime(2013, 10, 31, 13), 'en', 10)
      >>> pprint(d)
      {'en': {datetime.date(2013, 10, 31): {'hours': {12: 100, 13: 10},
                                            'total': 110}}}
      >>> update(d, datetime.datetime(2013, 10, 30, 12), 'en', 200)
      >>> pprint(d)
      {'en': {datetime.date(2013, 10, 30): {'hours': {12: 200}, 'total': 200},
              datetime.date(2013, 10, 31): {'hours': {12: 100, 13: 10},
                                            'total': 110}}}

      Setting a new value for an hour already in the data silently replaces
      the old value:

      >>> update(d, datetime.datetime(2013, 10, 31, 12, 2, 2), 'en', 300)
      >>> pprint(d)
      {'en': {datetime.date(2013, 10, 30): {'hours': {12: 200}, 'total': 200},
              datetime.date(2013, 10, 31): {'hours': {12: 300, 13: 10},
                                            'total': 310}}}'''
   date = timestamp.date()
   hour = timestamp.hour
   md.setdefault(proj, dict())
   md[proj].setdefault(date, { 'total': None, 'hours': dict() })
   md[proj][date]['hours'][hour] = count
   md[proj][date]['total'] = sum(md[proj][date]['hours'].itervalues())

### Bootstrap ###

try:
   args = u.parse_args(ap)
   if (__name__ == '__main__'):
      main()
except testable.Unittests_Only_Exception:
   testable.register('')
