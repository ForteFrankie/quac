#!/bin/bash

# Preprocess the Wikipedia data and test that it worked.

# Copyright (c) 2012-2013 Los Alamos National Security, LLC, and others.

. ./environment.sh
set -e
exec 2>&1

MAKEFILE='../../../misc/wp-preprocess.mk'

cd wp-access

# pprint pickled content
dump-metadata () {
    python <<EOF
import u
import pprint
pprint.pprint(u.pickle_load('metadata'))
EOF
}

# Does xargs() in the makefile work?
y "make -f $MAKEFILE xargs-test"

# Do we have the expected raw files?
x ls -R raw

# Remove any previously preprocessed files.
y "make -f $MAKEFILE clean"
x ls

# Run the make job.
y "make -f $MAKEFILE MDARGS=--notimes HASHMOD=4 XARGS_BLOCK=128"

# Check output.
y "for i in hashed/*; do echo -n \$i ' '; ls \$i | wc -l; done"
y "ls -l hashed/* hashed_small/* hashed_tiny/* | sed -E 's/^l.+:[0-9]{2} //'"
x md5sum metadata
dump-metadata | md5sum
dump-metadata

# Check updating
x touch raw/2012/2012-10/pagecounts-20121029-010001.gz
y "make -f $MAKEFILE MDARGS=--notimes HASHMOD=4"
x md5sum metadata
dump-metadata | md5sum
