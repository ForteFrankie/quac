#!/bin/bash

# Test that wp-get-dumps script works. To do so, we download dumps for one of
# the smallest Wikipedias, Afar (http://aa.wikipedia.org).
#
# This is a little risky, as mirror could stop working, the dump content could
# change, etc. But, we'll try it out. In this particular case, the wiki is
# "locked" as of 6/3/2013 (because there's no actual content), so that should
# help.
#
# Copyright (c) Los Alamos National Security, LLC, and others.

. ./environment.sh

cd $DATADIR

mkdir wp-dumps
cat > quacrc <<EOF
[wkpd]
dump_dir = wp-dumps
bandwidth_limit = 10000
projects = aawiki
dumps = stub-meta-history.xml.gz pagelinks.sql.gz
EOF

# Do the mirror. The head pipe is to remove the statistics, which change; the
# "updating Wikimedia dumps" line also changes b/c it contains $DESTDIR.
y "wp-get-dumps --notimes --config quacrc 2>&1 | head -n -15 | fgrep -v 'updating Wikimedia dumps'"

# Dumps contain timestamps and system version information which change.
# Therefore, we need to extract the .gz files and do a little magic for a
# proper comparison.
x cd wp-dumps/aawiki/latest
y "zcat aawiki-latest-pagelinks.sql.gz | egrep -v '^--' > aawiki-latest-pagelinks.sql"
y "zcat aawiki-latest-stub-meta-history.xml.gz | fgrep -v '<generator>' > aawiki-latest-stub-meta-history.xml"
x head -5 aawiki-latest-pagelinks.sql
x head -5 aawiki-latest-stub-meta-history.xml
x md5sum aawiki-latest-pagelinks.sql aawiki-latest-stub-meta-history.xml
